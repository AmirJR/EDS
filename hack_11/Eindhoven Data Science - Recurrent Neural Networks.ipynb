{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_signals(filename):\n",
    "    with open(filename, 'r') as fp:\n",
    "        data = fp.read().splitlines()\n",
    "        data = map(lambda x: x.rstrip().lstrip().split(), data)\n",
    "        data = [list(map(float, line)) for line in data]\n",
    "        data = np.array(data, dtype=np.float32)\n",
    "    return data\n",
    "\n",
    "def read_labels(filename):        \n",
    "    with open(filename, 'r') as fp:\n",
    "        activities = fp.read().splitlines()\n",
    "        activities = list(map(int, activities))\n",
    "    return np.array(activities)\n",
    "\n",
    "def randomize(dataset, labels):\n",
    "    permutation = np.random.permutation(labels.shape[0])\n",
    "    shuffled_dataset = dataset[permutation, :, :]\n",
    "    shuffled_labels = labels[permutation]\n",
    "    return shuffled_dataset, shuffled_labels\n",
    "\n",
    "def one_hot_encode(np_array, num_labels):\n",
    "    return (np.arange(num_labels) == np_array[:,None]).astype(np.float32)\n",
    "\n",
    "def reformat_data(dataset, labels):\n",
    "    no_labels = len(np.unique(labels))\n",
    "    labels = one_hot_encode(labels, no_labels)\n",
    "    dataset, labels = randomize(dataset, labels)\n",
    "    return dataset, labels\n",
    "\n",
    "d_activity_num_to_labels = {\n",
    "    1: 'walking',\n",
    "    2: 'walking upstairs',\n",
    "    3: 'walking downstairs',\n",
    "    4: 'sitting',\n",
    "    5: 'standing',\n",
    "    6: 'laying'\n",
    "}\n",
    "\n",
    "def accuracy(y_predicted, y):\n",
    "    return (100.0 * np.sum(np.argmax(y_predicted, 1) == np.argmax(y, 1)) / y_predicted.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The train dataset contains 7352 signals, each one of length 128 and 9 components \n",
      "The test dataset contains 2947 signals, each one of length 128 and 9 components \n",
      "The train dataset contains 7352 labels, with the following distribution:\n",
      " Counter({6: 1407, 5: 1374, 4: 1286, 1: 1226, 2: 1073, 3: 986})\n",
      "The test dataset contains 2947 labels, with the following distribution:\n",
      " Counter({6: 537, 5: 532, 1: 496, 4: 491, 2: 471, 3: 420})\n"
     ]
    }
   ],
   "source": [
    "INPUT_FOLDER_TRAIN = './data/UCI_HAR/train/InertialSignals/'\n",
    "INPUT_FOLDER_TEST = './data/UCI_HAR/test/InertialSignals/'\n",
    "\n",
    "INPUT_FILES_TRAIN = ['body_acc_x_train.txt', 'body_acc_y_train.txt', 'body_acc_z_train.txt', \n",
    "                     'body_gyro_x_train.txt', 'body_gyro_y_train.txt', 'body_gyro_z_train.txt',\n",
    "                     'total_acc_x_train.txt', 'total_acc_y_train.txt', 'total_acc_z_train.txt']\n",
    "\n",
    "INPUT_FILES_TEST = ['body_acc_x_test.txt', 'body_acc_y_test.txt', 'body_acc_z_test.txt', \n",
    "                     'body_gyro_x_test.txt', 'body_gyro_y_test.txt', 'body_gyro_z_test.txt',\n",
    "                     'total_acc_x_test.txt', 'total_acc_y_test.txt', 'total_acc_z_test.txt']\n",
    "\n",
    "LABELFILE_TRAIN = './data/UCI_HAR/train/y_train.txt'\n",
    "LABELFILE_TEST = './data/UCI_HAR/test/y_test.txt'\n",
    "\n",
    "train_signals, test_signals = [], []\n",
    "\n",
    "for input_file in INPUT_FILES_TRAIN:\n",
    "    signal = read_signals(INPUT_FOLDER_TRAIN + input_file)\n",
    "    train_signals.append(signal)\n",
    "train_signals = np.transpose(np.array(train_signals), (1, 2, 0))\n",
    "\n",
    "for input_file in INPUT_FILES_TEST:\n",
    "    signal = read_signals(INPUT_FOLDER_TEST + input_file)\n",
    "    test_signals.append(signal)\n",
    "test_signals = np.transpose(np.array(test_signals), (1, 2, 0))\n",
    "\n",
    "train_labels = read_labels(LABELFILE_TRAIN)\n",
    "test_labels = read_labels(LABELFILE_TEST)\n",
    "\n",
    "[no_signals_train, no_steps_train, no_components_train] = np.shape(train_signals)\n",
    "[no_signals_test, no_steps_test, no_components_test] = np.shape(test_signals)\n",
    "no_labels = len(np.unique(train_labels[:]))\n",
    "\n",
    "print(\"The train dataset contains {} signals, each one of length {} and {} components \".format(no_signals_train, no_steps_train, no_components_train))\n",
    "print(\"The test dataset contains {} signals, each one of length {} and {} components \".format(no_signals_test, no_steps_test, no_components_test))\n",
    "print(\"The train dataset contains {} labels, with the following distribution:\\n {}\".format(np.shape(train_labels)[0], Counter(train_labels[:])))\n",
    "print(\"The test dataset contains {} labels, with the following distribution:\\n {}\".format(np.shape(test_labels)[0], Counter(test_labels[:])))\n",
    "\n",
    "train_dataset, train_labels = reformat_data(train_signals, train_labels)\n",
    "test_dataset, test_labels = reformat_data(test_signals, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Defining some variables and models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_steps = 128\n",
    "num_components = 9\n",
    "num_labels = 6\n",
    "num_hidden = 32\n",
    "learning_rate = 0.001\n",
    "\n",
    "total_steps = 10000\n",
    "display_step = 100\n",
    "batch_size = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Fully Connected NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "NUM_HIDDEN = 32\n",
    "NUM_COMPONENTS = 9\n",
    "NUM_STEPS = 128\n",
    "\n",
    "def fccd_variables(num_labels, num_components = NUM_COMPONENTS, num_steps = NUM_STEPS, num_hidden = NUM_HIDDEN):\n",
    "    w1 = tf.Variable(tf.truncated_normal([num_components*num_steps, num_hidden], stddev=0.1))\n",
    "    b1 = tf.Variable(tf.constant(1.0, shape = [num_hidden]))\n",
    "   \n",
    "    w2 = tf.Variable(tf.truncated_normal([num_hidden, num_labels], stddev=0.1))\n",
    "    b2 = tf.Variable(tf.constant(1.0, shape = [num_labels]))\n",
    "    return {'w1': w1, 'b1': b1, 'w2': w2, 'b2': b2 }\n",
    "\n",
    "def fccd_model(data, variables):\n",
    "    shape = data.get_shape().as_list()\n",
    "    flattened_data = tf.reshape(data, [-1, shape[1]*shape[2]])\n",
    "    \n",
    "    layer1 = tf.matmul(flattened_data, variables['w1']) + variables['b1']\n",
    "    layer1_actv = tf.nn.relu(layer1)\n",
    "    \n",
    "    logit = tf.matmul(layer1_actv, variables['w2']) + variables['b2']\n",
    "    return logit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Recurrent NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "NUM_HIDDEN = 32\n",
    "\n",
    "def rnn_variables(num_labels, num_hidden = NUM_HIDDEN):\n",
    "    w1 = tf.Variable(tf.truncated_normal([num_hidden, num_labels]))\n",
    "    b1 = tf.Variable(tf.random_normal([num_labels]))\n",
    "    return {'w1': w1, 'b1': b1}\n",
    "\n",
    "def rnn_model(data, variables, num_hidden = NUM_HIDDEN):\n",
    "    splitted_data = tf.unstack(data, axis=1)\n",
    "    \n",
    "    #First line is for free, but the rest of the RNN you have to build yourself.\n",
    "    \n",
    "    return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Building the Graph with all computational steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_data = train_dataset\n",
    "train_labels = train_labels\n",
    "test_data = test_dataset\n",
    "test_labels = test_labels\n",
    "\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    #1) First we put the input data in a tensorflow friendly form.    \n",
    "    tf_dataset = tf.placeholder(tf.float32, shape=(None, num_steps, num_components))\n",
    "    tf_labels = tf.placeholder(tf.float32, shape = (None, num_labels))\n",
    "\n",
    "    #2) Choose the 'variables' containing the weights and biases\n",
    "    #variables = rnn_variables(num_labels)\n",
    "    variables = fccd_variables(num_labels)\n",
    "\n",
    "    #3.Choose the model you will use to calculate the logits (predicted labels)\n",
    "    #model = rnn_model\n",
    "    model = fccd_model\n",
    "    logits = model(tf_dataset, variables)\n",
    "\n",
    "    #4. Then we compute the softmax cross entropy between the logits and the (actual) labels\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=tf_labels))\n",
    "\n",
    "    #5. The optimizer is used to calculate the gradients of the loss function \n",
    "    #optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)\n",
    "    #optimizer = tf.train.MomentumOptimizer(learning_rate, momentum=0.0).minimize(loss)\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss)\n",
    "\n",
    "    # Predictions for the training, validation, and test data.\n",
    "    prediction = tf.nn.softmax(logits)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Running the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized with learning_rate 0.001\n",
      "step 0000 : loss is 001.45, accuracy on training set 14.3 %, accuracy on test set 15.61 %\n",
      "step 0100 : loss is 000.64, accuracy on training set 60.9 %, accuracy on test set 50.53 %\n",
      "step 0200 : loss is 000.45, accuracy on training set 68.3 %, accuracy on test set 58.64 %\n",
      "step 0300 : loss is 000.37, accuracy on training set 70.6 %, accuracy on test set 61.15 %\n",
      "step 0400 : loss is 000.36, accuracy on training set 68.8 %, accuracy on test set 61.15 %\n",
      "step 0500 : loss is 000.33, accuracy on training set 71.2 %, accuracy on test set 58.77 %\n",
      "step 0600 : loss is 000.32, accuracy on training set 70.3 %, accuracy on test set 58.64 %\n",
      "step 0700 : loss is 000.47, accuracy on training set 66.5 %, accuracy on test set 56.57 %\n",
      "step 0800 : loss is 000.72, accuracy on training set 59.1 %, accuracy on test set 50.22 %\n",
      "step 0900 : loss is 000.90, accuracy on training set 50.6 %, accuracy on test set 42.96 %\n",
      "step 1000 : loss is 001.04, accuracy on training set 51.6 %, accuracy on test set 39.19 %\n",
      "step 1100 : loss is 001.07, accuracy on training set 41.8 %, accuracy on test set 24.13 %\n",
      "step 1200 : loss is 001.25, accuracy on training set 34.5 %, accuracy on test set 23.38 %\n",
      "step 1300 : loss is 001.16, accuracy on training set 48.8 %, accuracy on test set 35.19 %\n",
      "step 1400 : loss is 001.17, accuracy on training set 44.5 %, accuracy on test set 38.85 %\n",
      "step 1500 : loss is 001.17, accuracy on training set 37.1 %, accuracy on test set 22.09 %\n",
      "step 1600 : loss is 001.17, accuracy on training set 45.1 %, accuracy on test set 37.46 %\n",
      "step 1700 : loss is 001.21, accuracy on training set 39.7 %, accuracy on test set 31.93 %\n",
      "step 1800 : loss is 001.18, accuracy on training set 45.5 %, accuracy on test set 34.34 %\n",
      "step 1900 : loss is 002.40, accuracy on training set 33.3 %, accuracy on test set 21.62 %\n",
      "step 2000 : loss is 001.25, accuracy on training set 43.5 %, accuracy on test set 36.65 %\n",
      "step 2100 : loss is 001.26, accuracy on training set 49.9 %, accuracy on test set 37.53 %\n",
      "step 2200 : loss is 001.82, accuracy on training set 24.9 %, accuracy on test set 27.55 %\n",
      "step 2300 : loss is 001.32, accuracy on training set 35.2 %, accuracy on test set 26.03 %\n",
      "step 2400 : loss is 001.31, accuracy on training set 43.0 %, accuracy on test set 32.13 %\n",
      "step 2500 : loss is 001.26, accuracy on training set 46.8 %, accuracy on test set 37.19 %\n",
      "step 2600 : loss is 001.33, accuracy on training set 40.2 %, accuracy on test set 36.04 %\n",
      "step 2700 : loss is 001.35, accuracy on training set 36.7 %, accuracy on test set 29.93 %\n",
      "step 2800 : loss is 001.32, accuracy on training set 39.4 %, accuracy on test set 35.02 %\n",
      "step 2900 : loss is 001.24, accuracy on training set 40.2 %, accuracy on test set 18.53 %\n",
      "step 3000 : loss is 001.37, accuracy on training set 38.8 %, accuracy on test set 27.04 %\n",
      "step 3100 : loss is 001.40, accuracy on training set 42.1 %, accuracy on test set 37.33 %\n",
      "step 3200 : loss is 001.72, accuracy on training set 29.8 %, accuracy on test set 18.60 %\n",
      "step 3300 : loss is 002.75, accuracy on training set 21.5 %, accuracy on test set 24.23 %\n",
      "step 3400 : loss is 001.36, accuracy on training set 40.1 %, accuracy on test set 29.18 %\n",
      "step 3500 : loss is 001.55, accuracy on training set 35.3 %, accuracy on test set 36.24 %\n",
      "step 3600 : loss is 001.33, accuracy on training set 44.1 %, accuracy on test set 36.34 %\n",
      "step 3700 : loss is 003.27, accuracy on training set 20.0 %, accuracy on test set 25.76 %\n",
      "step 3800 : loss is 002.01, accuracy on training set 34.8 %, accuracy on test set 26.26 %\n",
      "step 3900 : loss is 002.16, accuracy on training set 38.0 %, accuracy on test set 23.04 %\n",
      "step 4000 : loss is 001.44, accuracy on training set 46.5 %, accuracy on test set 34.54 %\n",
      "step 4100 : loss is 001.92, accuracy on training set 37.6 %, accuracy on test set 27.66 %\n",
      "step 4200 : loss is 001.77, accuracy on training set 37.8 %, accuracy on test set 29.93 %\n",
      "step 4300 : loss is 002.54, accuracy on training set 29.2 %, accuracy on test set 25.92 %\n",
      "step 4400 : loss is 001.61, accuracy on training set 34.6 %, accuracy on test set 34.27 %\n",
      "step 4500 : loss is 001.89, accuracy on training set 34.0 %, accuracy on test set 21.17 %\n",
      "step 4600 : loss is 001.60, accuracy on training set 34.7 %, accuracy on test set 38.89 %\n",
      "step 4700 : loss is 003.71, accuracy on training set 26.1 %, accuracy on test set 31.93 %\n",
      "step 4800 : loss is 002.92, accuracy on training set 22.0 %, accuracy on test set 20.12 %\n",
      "step 4900 : loss is 003.58, accuracy on training set 21.4 %, accuracy on test set 26.23 %\n",
      "step 5000 : loss is 002.72, accuracy on training set 35.3 %, accuracy on test set 18.09 %\n",
      "step 5100 : loss is 001.90, accuracy on training set 37.6 %, accuracy on test set 30.98 %\n",
      "step 5200 : loss is 001.60, accuracy on training set 46.6 %, accuracy on test set 39.63 %\n",
      "step 5300 : loss is 001.75, accuracy on training set 30.6 %, accuracy on test set 27.28 %\n",
      "step 5400 : loss is 004.18, accuracy on training set 33.6 %, accuracy on test set 35.36 %\n",
      "step 5500 : loss is 002.59, accuracy on training set 20.9 %, accuracy on test set 22.84 %\n",
      "step 5600 : loss is 002.45, accuracy on training set 23.2 %, accuracy on test set 27.35 %\n",
      "step 5700 : loss is 002.46, accuracy on training set 24.7 %, accuracy on test set 16.80 %\n",
      "step 5800 : loss is 002.27, accuracy on training set 32.9 %, accuracy on test set 21.58 %\n",
      "step 5900 : loss is 002.65, accuracy on training set 34.3 %, accuracy on test set 25.96 %\n",
      "step 6000 : loss is 002.36, accuracy on training set 44.7 %, accuracy on test set 28.64 %\n",
      "step 6100 : loss is 003.50, accuracy on training set 23.8 %, accuracy on test set 17.03 %\n",
      "step 6200 : loss is 003.29, accuracy on training set 34.7 %, accuracy on test set 20.12 %\n",
      "step 6300 : loss is 002.22, accuracy on training set 33.9 %, accuracy on test set 19.38 %\n",
      "step 6400 : loss is 002.83, accuracy on training set 33.0 %, accuracy on test set 18.87 %\n",
      "step 6500 : loss is 002.71, accuracy on training set 30.9 %, accuracy on test set 19.75 %\n",
      "step 6600 : loss is 002.61, accuracy on training set 37.6 %, accuracy on test set 20.46 %\n",
      "step 6700 : loss is 002.54, accuracy on training set 37.4 %, accuracy on test set 28.37 %\n",
      "step 6800 : loss is 002.83, accuracy on training set 36.9 %, accuracy on test set 35.46 %\n",
      "step 6900 : loss is 003.45, accuracy on training set 28.6 %, accuracy on test set 23.79 %\n",
      "step 7000 : loss is 002.89, accuracy on training set 27.6 %, accuracy on test set 26.06 %\n",
      "step 7100 : loss is 002.51, accuracy on training set 32.3 %, accuracy on test set 37.12 %\n",
      "step 7200 : loss is 003.41, accuracy on training set 31.0 %, accuracy on test set 28.30 %\n",
      "step 7300 : loss is 002.49, accuracy on training set 33.9 %, accuracy on test set 26.50 %\n",
      "step 7400 : loss is 002.19, accuracy on training set 35.7 %, accuracy on test set 18.22 %\n",
      "step 7500 : loss is 002.27, accuracy on training set 43.9 %, accuracy on test set 29.05 %\n",
      "step 7600 : loss is 005.21, accuracy on training set 27.4 %, accuracy on test set 24.26 %\n",
      "step 7700 : loss is 002.87, accuracy on training set 37.2 %, accuracy on test set 31.93 %\n",
      "step 7800 : loss is 004.38, accuracy on training set 22.6 %, accuracy on test set 27.82 %\n",
      "step 7900 : loss is 003.31, accuracy on training set 34.2 %, accuracy on test set 27.66 %\n",
      "step 8000 : loss is 002.82, accuracy on training set 34.8 %, accuracy on test set 27.42 %\n",
      "step 8100 : loss is 004.02, accuracy on training set 26.6 %, accuracy on test set 20.53 %\n",
      "step 8200 : loss is 002.91, accuracy on training set 32.9 %, accuracy on test set 18.97 %\n",
      "step 8300 : loss is 003.08, accuracy on training set 34.7 %, accuracy on test set 20.70 %\n",
      "step 8400 : loss is 005.42, accuracy on training set 24.7 %, accuracy on test set 27.59 %\n",
      "step 8500 : loss is 004.46, accuracy on training set 26.7 %, accuracy on test set 32.13 %\n",
      "step 8600 : loss is 004.26, accuracy on training set 37.8 %, accuracy on test set 24.02 %\n",
      "step 8700 : loss is 003.12, accuracy on training set 37.7 %, accuracy on test set 24.06 %\n",
      "step 8800 : loss is 004.64, accuracy on training set 24.2 %, accuracy on test set 17.81 %\n",
      "step 8900 : loss is 006.34, accuracy on training set 27.0 %, accuracy on test set 29.89 %\n",
      "step 9000 : loss is 004.95, accuracy on training set 28.8 %, accuracy on test set 23.58 %\n",
      "step 9100 : loss is 003.78, accuracy on training set 33.6 %, accuracy on test set 26.33 %\n",
      "step 9200 : loss is 004.17, accuracy on training set 28.7 %, accuracy on test set 30.71 %\n",
      "step 9300 : loss is 004.95, accuracy on training set 27.1 %, accuracy on test set 17.10 %\n",
      "step 9400 : loss is 003.32, accuracy on training set 23.1 %, accuracy on test set 27.96 %\n",
      "step 9500 : loss is 003.25, accuracy on training set 33.8 %, accuracy on test set 19.55 %\n",
      "step 9600 : loss is 006.25, accuracy on training set 34.1 %, accuracy on test set 33.73 %\n",
      "step 9700 : loss is 005.98, accuracy on training set 26.2 %, accuracy on test set 18.39 %\n",
      "step 9800 : loss is 004.36, accuracy on training set 18.9 %, accuracy on test set 35.93 %\n",
      "step 9900 : loss is 003.58, accuracy on training set 30.3 %, accuracy on test set 33.49 %\n"
     ]
    }
   ],
   "source": [
    "with tf.Session(graph=graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print('Initialized with learning_rate', learning_rate)\n",
    "    for step in range(total_steps):\n",
    "        #Since we are using stochastic gradient descent, we are selecting  small batches from the training dataset,\n",
    "        #and training the convolutional neural network each time with a batch. \n",
    "        offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "        batch_data = train_data[offset:(offset + batch_size), :, :]\n",
    "        batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "\n",
    "        feed_dict = {tf_dataset : batch_data, tf_labels : batch_labels}\n",
    "        _, l, train_predictions = session.run([optimizer, loss, prediction], feed_dict=feed_dict)\n",
    "        train_accuracy = accuracy(train_predictions, batch_labels)\n",
    "\n",
    "        if step % display_step == 0:\n",
    "            feed_dict = {tf_dataset : test_data, tf_labels : test_labels}\n",
    "            _, test_predictions = session.run([loss, prediction], feed_dict=feed_dict)\n",
    "            test_accuracy = accuracy(test_predictions, test_labels)\n",
    "            message = \"step {:04d} : loss is {:06.2f}, accuracy on training set {} %, accuracy on test set {:02.2f} %\".format(step, l, train_accuracy, test_accuracy)\n",
    "            print(message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
